{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe+TwQxE0LdvVL7sMqFz2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikhila1619/Excler-/blob/main/Day_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_numeric\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Ensure necessary resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Download the missing 'punkt_tab' resource\n",
        "\n",
        "# Function for preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and stopwords\n",
        "    tokens = [strip_punctuation(token) for token in tokens if token not in string.punctuation]\n",
        "    tokens = [remove_stopwords(token) for token in tokens]\n",
        "\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Apply lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Read sample text from file (replace 'sample.txt' with your text file path)\n",
        "with open('sample.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Preprocess the text\n",
        "processed_text = preprocess_text(text)\n",
        "\n",
        "# Show the processed tokens\n",
        "print(\"Processed Tokens:\", processed_text)\n",
        "\n",
        "# Create a dictionary and corpus using Gensim\n",
        "dictionary = corpora.Dictionary([processed_text])\n",
        "corpus = [dictionary.doc2bow(processed_text)]\n",
        "\n",
        "# Show the dictionary and corpus\n",
        "print(\"Dictionary:\", dictionary.token2id)\n",
        "print(\"Corpus:\", corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528_YV-TA863",
        "outputId": "8274e12b-a3f1-413c-c832-7b54a380b935"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Tokens: ['process', 'token', 'natur', 'languag', 'process', 'subfield', 'artifici', 'intellig', 'focu', 'interact', 'comput', 'human', 'use', 'natur', 'languag', 'dictionari', 'natur', '0', 'languag', '1', 'process', '2', 'subfield', '3', 'artifici', '4', 'intellig', '5', 'focu', '6', 'interact', '7', 'comput', '8', 'human', '9', 'use', '10', 'corpu', '0', '1', '1', '1', '2', '1', '3', '1', '4', '1', '5', '1', '6', '1', '7', '1', '8', '1', '9', '1', '10', '1']\n",
            "Dictionary: {'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'artifici': 11, 'comput': 12, 'corpu': 13, 'dictionari': 14, 'focu': 15, 'human': 16, 'intellig': 17, 'interact': 18, 'languag': 19, 'natur': 20, 'process': 21, 'subfield': 22, 'token': 23, 'use': 24}\n",
            "Corpus: [[(0, 2), (1, 13), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (9, 2), (10, 2), (11, 2), (12, 2), (13, 1), (14, 1), (15, 2), (16, 2), (17, 2), (18, 2), (19, 3), (20, 3), (21, 3), (22, 2), (23, 1), (24, 2)]]\n"
          ]
        }
      ]
    }
  ]
}